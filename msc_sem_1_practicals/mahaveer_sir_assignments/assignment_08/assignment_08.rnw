\documentclass[11pt, a4paper]{article}

\usepackage[top = 1 in, bottom = 1 in, left = 1 in, right = 1 in ]{geometry}

\usepackage{amsmath, amssymb, amsfonts}
\usepackage{enumerate}
\usepackage{array}
\usepackage{multirow}
\usepackage{dingbat}
\usepackage{fontawesome5}
\usepackage{tasks}
\usepackage{bbding}
\usepackage{twemojis}
% how to use bull's eye ----- \scalebox{2.0}{\twemoji{bullseye}}
\usepackage{fontspec}
\usepackage{customdice}
% how to put dice face ------ \dice{2}

\title{MSMS 106 : Assignment 08}
\author{Ananda Biswas}
\date{\today}

\newfontface\myfont{Myfont1-Regular.ttf}[LetterSpace=0.05em]
% how to use ---- {\setlength{\spaceskip}{1em plus 0.5em minus 0.5em} \fontsize{17}{20}\myfont --write text here-- \par}

\begin{document}

\maketitle


\section*{\faArrowAltCircleRight[regular] \textcolor{blue}{Objective}}

\hspace{1cm} To find Maximum Likelihood Estimate of the parameters of the Weibull Distribution given as follows :

$$f_X (x) = \dfrac{p}{\sigma} \,\, x^{p - 1} \,\, exp\left\{-\dfrac{x^p}{\sigma}\right\} \,\, I_{(0, \infty)}(x); \,\, p > 0, \,\, \sigma > 0 $$

and to compare ML estimates for different sample sizes. 



\section*{\faArrowAltCircleRight[regular] \textcolor{blue}{Theory}}

$p$ is called the shape parameter and $\sigma$ is called the scale parameter of the distribution. \\

For a sample of size $n$, the likelihood function is 

$$L(p, \sigma) = \left( \dfrac{p}{\sigma} \right)^{n} \cdot exp \left\{-\dfrac{1}{\sigma}\sum \limits_{i = 1}^{n} x_i^p \right\} \,\, \left( \prod \limits_{i = 1}^{n} x_i \right)^{p - 1}. $$


The log-likelihood function is 

$$ l(p, \sigma) = n \ln p - n \ln \sigma - \dfrac{1}{\sigma}\sum \limits_{i = 1}^{n} x_i^p + ({p - 1}) \sum \limits_{i = 1}^{n} \ln x_i .$$

The partial derivative of the log-likelihood w.r.t. $\sigma$ is

$$ \dfrac{\partial}{\partial \sigma} l(p, \sigma) = - \dfrac{n}{\sigma} + \dfrac{1}{\sigma^2}\sum \limits_{i = 1}^{n} x_i^p . $$

Setting it to $0$ and solving for $\sigma$ we get,

\begin{equation}
\hat{\sigma} =  \dfrac{1}{n}\sum \limits_{i = 1}^{n} x_i^p.
\end{equation}

The partial derivative of the log-likelihood w.r.t. $p$ is

\begin{align*}
\dfrac{\partial}{\partial p} l(p, \sigma) &= \dfrac{n}{p} - \dfrac{1}{\sigma} \sum \limits_{i = 1}^{n} (x_i^p \cdot \ln x_i) + \sum \limits_{i = 1}^{n} \ln x_i \\
&= \dfrac{n}{p} - \dfrac{n \cdot \sum \limits_{i = 1}^{n} (x_i^p \cdot \ln x_i)}{\sum \limits_{i = 1}^{n} x_i^p} + \sum \limits_{i = 1}^{n} \ln x_i.
\end{align*}

Setting the partial derivative w.r.t. $p$ to 0, we get an equation in $p$ given by

\begin{equation}
g(p) = \dfrac{n}{p} - \dfrac{n \cdot \sum \limits_{i = 1}^{n} (x_i^p \cdot \ln x_i)}{\sum \limits_{i = 1}^{n} x_i^p} + \sum \limits_{i = 1}^{n} \ln x_i = 0.
\end{equation}

We cannot obtain any closed-form solution of $g(p)$, so we opt for numerical solution. \\

\begin{equation}
g'(p) = - \dfrac{n}{p^2} - \dfrac{n \cdot \left(\sum \limits_{i = 1}^{n} x_i^p\right) \cdot \sum \limits_{i = 1}^{n} (x_i^p \cdot (\ln x_i)^2) - n \cdot  \left[ \sum \limits_{i = 1}^{n} (x_i^p \cdot \ln x_i) \right]^2}{\left(\sum \limits_{i = 1}^{n} x_i^p\right)^2}.
\end{equation}


Using $(2)$ and $(3)$, we get an approximate solution of $g(p)$ by \textbf{Newton-Raphson method}. That solution is indeed ML estimate of $p$. By using that estimate of $p$ in $(1)$, we will obtain ML estimate of $\sigma$. \\

$\bullet$ To compare MLEs from different sample sizes, we compare their MSEs. \\

For a fixed sample size $n$, $\text{MSE}(\hat{\theta}_{\text{MLE}}) = \dfrac{1}{k} \sum \limits_{i = 1}^{k} (\hat{\theta_i} - \theta_0)^2$, \\
where $\hat{\theta}_1$, $\hat{\theta}_2$, $\hat{\theta}_3$, $\ldots$, $\hat{\theta}_k$ are MLEs from different samples of fixed size $n$ and $\theta_0$ is the true value of $\theta$.

$\bullet$ To get random sample from Weibull distribution, we shall use the CDF Inversion technique. \\

We shall get Weibull random numbers from the following formula :

$$w = h(u) = (-\sigma \cdot \log (1 - u))^\frac{1}{p}$$

where $u$ is an Uniform$[0, 1]$ random number.


\section*{\faArrowAltCircleRight[regular] \textcolor{blue}{R Program}}

The following function takes random sample, initial approximation of $p$ and number of iteration as inputs and gives $\hat{p}$ and $\hat{\sigma}$ as outputs.

<<>>=
Weibull_MLE <- function(weibull_sample, shape_initial, n_iteration){
  
  p <- c(shape_initial)
  
  n <- length(weibull_sample)
  
  f1 <- function(p){

    result <- n / p -
              ( n * sum(weibull_sample^p * log(weibull_sample)) ) /
                        sum(weibull_sample^p) +
              sum(log(weibull_sample))

    return(result)
  }
  
  f2 <- function(p){

    temp1 <- n * sum(weibull_sample^p) *
                 sum(weibull_sample^p * log(weibull_sample)^2)

    temp2 <- n * sum(weibull_sample^p * log(weibull_sample))^2

    temp3 <- sum(weibull_sample^p)^2

    result <- - n / p^2 - (temp1 - temp2) / temp3

    return(result)
  }
  
  iterations <- n_iteration
  
  for (i in 2:iterations) {
    p[i] <- p[i-1] - f1(p[i-1]) / f2(p[i-1])
    
    if(abs(f1(p[length(p)])) < 0.001) break
  }
  
  p_hat <- p[length(p)]
 
  sigma_hat <- sum(weibull_sample^p_hat) / n
  
  return(c(p_hat, sigma_hat))
}
@

Now we calculate MLEs for different sample sizes.

<<>>=
n <- c(100, 200, 500, 1000, 5000, 10000, 100000)
@

<<>>=
estimated_shape <- c(); estimated_scale <- c()
@

<<>>=
for (i in 1:length(n)) {
  our_sample <- (- 3 * log(1 - runif(n[i])))^(1/5)
  temp <- Weibull_MLE(our_sample, shape_initial = 7, n_iteration = 1000)
  estimated_shape[i] <- temp[1]
  estimated_scale[i] <- temp[2]
}
@

<<>>=
Weibull_MLE_df1 <- data.frame(sample_size = n, 
                            shape_hat = estimated_shape, 
                            scale_hat = estimated_scale)
@

<<>>=
Weibull_MLE_df1
@

\smallpencil \hspace{0.1cm} {\setlength{\spaceskip}{1em plus 0.5em minus 0.5em} \fontsize{17}{20}\myfont As sample size increases, the estimates of parameters seem to converge at 5 and 3 respectively. \\

Now we shall compare the MSEs.
\par}




<<>>=
MSE_p_hat <- c(); MSE_sigma_hat <- c()
@

<<>>=
for(j in 1:length(n)){

  p_hats <- c(); sigma_hats <- c()
  
  for (i in 1:100) {
    a_sample <- (- 3 * log(1 - runif(n[j])))^(1/5)
    temp <- Weibull_MLE(a_sample, shape_initial = 7, n_iteration = 1000)
    p_hats[i] <- temp[1]
    sigma_hats[i] <- temp[2]
  }
  
  MSE_p_hat[j] <- mean( (p_hats - 5)^2 )
  
  MSE_sigma_hat[j] <- mean( (sigma_hats - 3)^2 )
}
@

<<>>=
Weibull_MLE_df2 <- data.frame(sample_size = n,
                            MSE_p_hat = MSE_p_hat,
                            MSE_sigma_hat = MSE_sigma_hat)
@

<<>>=
Weibull_MLE_df2
@


\section*{\faArrowAltCircleRight[regular] \textcolor{blue}{Conclusion}}

\smallpencil \hspace{0.3cm} {\setlength{\spaceskip}{1em plus 0.5em minus 0.5em} \fontsize{17}{20}\myfont As sample size increases, MSEs of both the parameters decrease monotonically. This implies MLEs give better estimates as sample size increases. For larger and larger samples, the MLEs will smoothly converge to the true values of the parameters respectively. \par}

\end{document}