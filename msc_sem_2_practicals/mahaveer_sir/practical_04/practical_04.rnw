\documentclass[11pt, a4paper]{article}

\usepackage[top = 1 in, bottom = 1 in, left = 1 in, right = 1 in ]{geometry}

\usepackage{amsmath, amssymb, amsfonts}
\usepackage{enumerate}
\usepackage{array}
\usepackage{multirow}
\usepackage{dingbat}
\usepackage{fontawesome5}
\usepackage{tasks}
\usepackage{bbding}
\usepackage{twemojis}
% how to use bull's eye ----- \scalebox{2.0}{\twemoji{bullseye}}
\usepackage{fontspec}
\usepackage{customdice}
% how to put dice face ------ \dice{2}

\title{MSMS 206 : Practical 04}
\author{Ananda Biswas}
\date{\today}

\newfontface\myfont{Myfont1-Regular.ttf}[LetterSpace=0.05em]
% how to use ---- {\setlength{\spaceskip}{1em plus 0.5em minus 0.5em} \fontsize{17}{20}\myfont --write text here-- \par}

\newfontface\cbfont{CaveatBrush-Regular.ttf}
% how to use --- \myfont --write text here--

\begin{document}

\maketitle


\section*{\faArrowAltCircleRight[regular] \textcolor{blue}{Question}}

\hspace{1cm} Consider a lifetime variable that follows a \textbf{Weibull distribution} with shape parameter $\alpha$ and scale parameter $\lambda$. The probability density function of this distribution is given by:

\[
f(t; \alpha, \lambda) = \frac{\alpha}{\lambda} \left( \frac{t}{\lambda} \right)^{\alpha - 1} e^{-(t/\lambda)^\alpha}, \quad t > 0.
\]

The objective is to evaluate the performance of \textbf{maximum likelihood estimation (MLE)} for different sample sizes. First, generate random samples of sizes $n = 60, 80, 100, 120$ and $140$ from this Weibull distribution. For each sample, estimate the parameters $\alpha$ and $\lambda$ using MLE.

Next, compute the \textbf{standard errors} of the ML estimates and evaluate their accuracy by estimating the \textbf{bias} and the \textbf{mean squared error (MSE)}. Also evaluate the distribution of MLE of each parameter. Discuss the asymptotic properties of MLE, particularly its consistency and efficiency, based on the observed results.




\section*{\faArrowAltCircleRight[regular] \textcolor{blue}{Build-up for obtaining MLE}}

For a sample of size $n$, the likelihood function is given by

\begin{align*}
L(\alpha, \lambda) &= \prod \limits_{i = 1}^{n} \frac{\alpha}{\lambda} \left( \frac{x_i}{\lambda} \right)^{\alpha - 1} e^{-(x_i / \lambda)^\alpha} \\
&= \left( \dfrac{\alpha}{\lambda} \right)^n \left( \prod \limits_{i = 1}^{n}  \dfrac{x_i}{\lambda} \right)^{\alpha - 1} \text{exp}\left\{- \sum \limits_{i = 1}^{n} \left( \dfrac{x_i}{\lambda}\ \right)^{\alpha} \right\}.
\end{align*}

\vspace{0.5cm}

The log-likelihood function is given by

$$l(\alpha, \lambda) = n \log \alpha - n \log \lambda + (\alpha - 1) \sum_{i=1}^{n} \log \left( \frac{x_i}{\lambda} \right) - \sum_{i=1}^{n} \left( \frac{x_i}{\lambda} \right)^{\alpha}.$$

Now, 
\begin{align*}
\dfrac{\partial}{\partial \alpha} l(\alpha, \lambda) &= \dfrac{n}{\alpha} + \sum \limits_{i=1}^{n} \log \left( \dfrac{x_i}{\lambda} \right) - \sum \limits_{i=1}^{n} \left( \dfrac{x_i}{\lambda} \right)^{\alpha} \log \left( \dfrac{x_i}{\lambda} \right) = u(\alpha, \lambda) \text{, say and}
\end{align*}

\newpage

\begin{align*}
\dfrac{\partial}{\partial \lambda} l(\alpha, \lambda) &= -\dfrac{n}{\lambda} + (\alpha - 1) \sum \limits_{i=1}^{n} \dfrac{\lambda}{x_i} \cdot \left( -\dfrac{x_i}{\lambda^2} \right) + \sum \limits_{i=1}^{n} \dfrac{\alpha \cdot x_i^{\alpha}}{\lambda^{\alpha + 1}}\\[0.3cm]
&= -\dfrac{n}{\lambda} - (\alpha - 1) \dfrac{n}{\lambda} + \sum \limits_{i=1}^{n} \dfrac{\alpha \cdot x_i^{\alpha}}{\lambda^{\alpha + 1}}\\[0.3cm]
&= -\dfrac{n \alpha}{\lambda} + \sum \limits_{i=1}^{n} \dfrac{\alpha \cdot x_i^{\alpha}}{\lambda^{\alpha + 1}} = v(\alpha, \lambda) \text{, say}.
\end{align*}

Solutions of the likelihood equations $u(\alpha, \lambda) = 0$ and $v(\alpha, \lambda) = 0$ can not be obtained in closed form. So we opt for numerical approach to get approximate solutions. \\

By Newton-Raphson method for system of non-linear equations, approximate solution $(\alpha_{k+1}, \lambda_{k+1})$ after $(k+1)$ iterations is given by
\begin{gather*}
  \begin{bmatrix} \alpha_{k+1} \\ \lambda_{k+1} \end{bmatrix} 
  =
  \begin{bmatrix} \alpha_{k} \\ \lambda_{k} \end{bmatrix}
  -
  \begin{bmatrix} u_{\alpha}(\alpha_k, \lambda_k) & u_{\lambda}(\alpha_k, \lambda_k) \\ v_{\alpha}(\alpha_k, \lambda_k) & v_{\lambda}(\alpha_k, \lambda_k)  \end{bmatrix} ^{-1}
  \cdot
  \begin{bmatrix} u(\alpha_k, \lambda_k) \\ v(\alpha_k, \lambda_k) \end{bmatrix}, \,\,\,\,\,\, k = 0, 1, 2, \ldots
\end{gather*}

with notations having their usual meanings. \\

Now,
\begin{align*}
u_{\alpha}(\alpha, \lambda) &= \dfrac{\partial}{\partial \alpha} u(\alpha, \lambda) = -\dfrac{n}{\alpha^2} - \sum \limits_{i=1}^{n} \left( \dfrac{x_i}{\lambda} \right)^{\alpha} \left[ \log \left( \dfrac{x_i}{\lambda} \right) \right]^2;\\[0.3cm]
u_{\lambda}(\alpha, \lambda) &= \dfrac{\partial}{\partial \lambda} u(\alpha, \lambda) =
\sum \limits_{i=1}^{n} \dfrac{\lambda}{x_i} \cdot \left( -\dfrac{x_i}{\lambda^2} \right)
- \sum \limits_{i=1}^{n} \left[\dfrac{(-\alpha) x_i^{\alpha}}{\lambda^{\alpha + 1}} \log \left( \dfrac{x_i}{\lambda} \right) + \left( \dfrac{x_i}{\lambda} \right)^{\alpha}\dfrac{\lambda}{x_i} \cdot \left( -\dfrac{x_i}{\lambda^2} \right) \right]\\[0.3cm]
&\phantom{= \frac{\partial}{\partial \lambda} u(\alpha, \lambda) \hspace{0.1cm}} = -\dfrac{n}{\lambda} + \sum \limits_{i=1}^{n} \left[\dfrac{x_i^{\alpha}}{\lambda^{\alpha + 1}} \left( \alpha \log \left( \dfrac{x_i}{\lambda} \right) + 1 \right) \right].
\end{align*}

Also,
\begin{align*}
v_{\alpha}(\alpha, \lambda) &= \dfrac{\partial}{\partial \alpha} v(\alpha, \lambda) =
-\dfrac{n}{\lambda} + \sum \limits_{i=1}^{n} \left[\dfrac{x_i^{\alpha}}{\lambda^{\alpha + 1}}
+ \dfrac{\alpha}{\lambda} \left( \dfrac{x_i}{\lambda} \right)^{\alpha} \log \left( \dfrac{x_i}{\lambda} \right) \right];\\[0.3cm]
v_{\lambda}(\alpha, \lambda) &= \dfrac{\partial}{\partial \lambda} v(\alpha, \lambda) =
\dfrac{n \alpha}{\lambda^2} - \sum \limits_{i=1}^{n} \dfrac{\alpha(\alpha + 1) x_i^{\alpha}}{\lambda^{\alpha + 2}}.
\end{align*}

\section*{\faArrowAltCircleRight[regular] \textcolor{blue}{Program}}

<<>>=
u <- function(alpha, lambda, s){
  
  a <- length(s) / alpha
  
  b <- sum(log(s / lambda))
  
  c <- sum((s / lambda)^alpha * log(s / lambda))
  
  return(a + b - c)
}
@

<<>>=
v <- function(alpha, lambda, s){
  
  a <- - (length(s) * alpha) / lambda
  
  b <- sum(alpha * s^alpha) / lambda^(alpha + 1)
  
  return(a + b)
}
@

<<>>=
u_alpha <- function(alpha, lambda, s){
  
  a <- - length(s) / alpha^2
  
  b <- sum((s / lambda)^alpha * log(s / lambda)^2)
  
  return(a - b)
}
@

<<>>=
u_lambda <- function(alpha, lambda, s){
  
  a <- - length(s) / lambda
  
  b <- sum(s^alpha * (alpha * log(s / lambda) + 1)) / lambda^(alpha + 1)
  
  return(a + b)
}
@

<<>>=
v_alpha1 <- function(alpha, lambda, s){

  a <- - length(s) / lambda

  b <- sum(s^alpha) / lambda^(alpha + 1)

  c <- sum( (s / lambda)^alpha * log(s / lambda) ) * (alpha / lambda)

  return(a + b + c)
}

v_alpha <- function(alpha, lambda, s) {
  a <- -length(s) / lambda
  z <- s / lambda
  b <- sum(z^alpha) / lambda^(alpha + 1)
  c <- sum(alpha * z^alpha * log(z)) / lambda^(alpha + 1)
  return(a + b + c)
}


@

<<>>=
v_lambda <- function(alpha, lambda, s){
  
  a <- (length(s) * alpha) / lambda^2
  
  b <- sum(s^alpha) * (alpha * (alpha + 1)) / lambda^(alpha + 2)
  
  return(a - b)
}
@

<<>>=
x <- rweibull(100, shape = 3, scale = 2)
@

<<>>=
df <- data.frame(alpha_hat = c(2), lambda_hat = c(1))
@

<<>>=
for (i in 1:20) {

  a <- df$alpha_hat[i]; l <- df$lambda_hat[i]
  
  A <- matrix(c(a, l), nrow = 2, byrow = TRUE)
  
  J <- matrix(c(u_alpha(a, l, x), u_lambda(a, l, x),
                v_alpha(a, l, x), v_lambda(a, l, x)), nrow = 2, ncol = 2, byrow = TRUE)
  
  B <- matrix(c(u(a, l, x), v(a, l, x)), nrow = 2, byrow = TRUE)
 
  new <- A - solve(J) %*% B
 
  df[(i+1),] <- c(new[1,], new[2,])
}
@

<<>>=
df
@

\smallpencil \hspace{1cm} % Start writing from here

\end{document}